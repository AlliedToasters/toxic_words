{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challange: Build NLP Model\n",
    "<br>\n",
    "I am using <a href='https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge'>this Kaggle competition</a> to build an NLP model. The task is to identify \"toxic comments\" and the hand-labeled data is provided by Jigsaw. The challenge is not only to find toxic comments but to correctly identify the type of toxic comment as given in six classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "32       1             0        1       0       1              0\n",
       "81       1             0        1       0       1              0\n",
       "86       1             0        1       0       1              0\n",
       "104      1             0        1       0       1              0\n",
       "122      0             0        1       0       1              0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "classes = [\n",
    "    'toxic',\n",
    "    'severe_toxic',\n",
    "    'obscene', \n",
    "    'threat',\n",
    "    'insult', \n",
    "    'identity_hate'\n",
    "]\n",
    "\n",
    "df[(df[classes].sum(axis=1) > 1)][classes].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the classes are not mutually exclusive.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The model performance is the mean column-wise log-loss score for the six distinct toxic comment types. To calculate this, one must calculate the log loss of each prediction for each class and take the mean of those six log-loss values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preparation\n",
    "For a minimal first-attempt, I'll use spacy to parse the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = spacy.load('en')\n",
    "data = pd.DataFrame(index=df.index)\n",
    "\n",
    "\n",
    "def parse(document):\n",
    "    iteration = next(stat)\n",
    "    if iteration % 10000 == 0:\n",
    "        print(iteration, \" of {} rows complete!\".format(len(df)))\n",
    "    return prs(document)\n",
    "\n",
    "stat = iter(range(0, len(df)))\n",
    "data['raw_parse'] = df.comment_text.apply(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\n",
       "\n",
       " One more feeder league? \n",
       "\n",
       "If I am correct, there should be one more league added to the Relegation to table template. The league in question is the Aldershot Senior League. Please view this talk page for further details under section 18 (Error?) of the page. STalk to me \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.raw_parse.iloc[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(document):\n",
    "    iteration = next(stat)\n",
    "    if iteration % 10000 == 0:\n",
    "        print(iteration, \" of {} rows complete!\".format(len(df)))\n",
    "    lemmas = []\n",
    "    for token in document:\n",
    "        if not token.is_punct and not token.is_space and not token.is_oov:\n",
    "            lemmas.append(token.lemma_)\n",
    "    return lemmas\n",
    "\n",
    "def get_oov(document):\n",
    "    iteration = next(stat)\n",
    "    if iteration % 10000 == 0:\n",
    "        print(iteration, \" of {} rows complete!\".format(len(df)))\n",
    "    oovs = []\n",
    "    for token in document:\n",
    "        if token.is_oov:\n",
    "            oovs.append(token.orth_)\n",
    "    return oovs\n",
    "\n",
    "def reconstruct_text(lemmas):\n",
    "    iteration = next(stat)\n",
    "    if iteration % 10000 == 0:\n",
    "        print(iteration, \" of {} rows complete!\".format(len(df)))\n",
    "    result = ''\n",
    "    for lemma in lemmas:\n",
    "        if str(lemma) == '-PRON-':\n",
    "            result += ' ' + 'pronoun'\n",
    "        else:\n",
    "            result += ' ' + str(lemma)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  of 95851 rows complete!\n",
      "5000  of 95851 rows complete!\n",
      "10000  of 95851 rows complete!\n",
      "15000  of 95851 rows complete!\n",
      "20000  of 95851 rows complete!\n",
      "25000  of 95851 rows complete!\n",
      "30000  of 95851 rows complete!\n",
      "35000  of 95851 rows complete!\n",
      "40000  of 95851 rows complete!\n",
      "45000  of 95851 rows complete!\n",
      "50000  of 95851 rows complete!\n",
      "55000  of 95851 rows complete!\n",
      "60000  of 95851 rows complete!\n",
      "65000  of 95851 rows complete!\n",
      "70000  of 95851 rows complete!\n",
      "75000  of 95851 rows complete!\n",
      "80000  of 95851 rows complete!\n",
      "85000  of 95851 rows complete!\n",
      "90000  of 95851 rows complete!\n",
      "95000  of 95851 rows complete!\n"
     ]
    }
   ],
   "source": [
    "stat = iter(range(0, len(df)))\n",
    "data['lemmas'] = data.raw_parse.apply(get_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  of 95851 rows complete!\n",
      "10000  of 95851 rows complete!\n",
      "20000  of 95851 rows complete!\n",
      "30000  of 95851 rows complete!\n",
      "40000  of 95851 rows complete!\n",
      "50000  of 95851 rows complete!\n",
      "60000  of 95851 rows complete!\n",
      "70000  of 95851 rows complete!\n",
      "80000  of 95851 rows complete!\n",
      "90000  of 95851 rows complete!\n"
     ]
    }
   ],
   "source": [
    "stat = iter(range(0, len(df)))\n",
    "data['oovs'] = data.raw_parse.apply(get_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  of 95851 rows complete!\n",
      "10000  of 95851 rows complete!\n",
      "20000  of 95851 rows complete!\n",
      "30000  of 95851 rows complete!\n",
      "40000  of 95851 rows complete!\n",
      "50000  of 95851 rows complete!\n",
      "60000  of 95851 rows complete!\n",
      "70000  of 95851 rows complete!\n",
      "80000  of 95851 rows complete!\n",
      "90000  of 95851 rows complete!\n"
     ]
    }
   ],
   "source": [
    "stat = iter(range(0, len(df)))\n",
    "data['clean_text'] = data.lemmas.apply(reconstruct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' one more feeder league if pronoun be correct there should be one more league add to the relegation to table template the league in question be the senior league please view this talk page for further detail under section 18 error of the page to pronoun'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.clean_text[1234]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "vctr = CountVectorizer(ngram_range=(1, 2), stop_words = 'english', min_df=.001, max_df=.5)\n",
    "X = vctr.fit_transform(data.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_scorer = make_scorer(log_loss)\n",
    "Y = df.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.29605676,  2.35342502,  2.54585443])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "cross_val_score(mod, X, Y, scoring=ll_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.34134637,  3.27118427,  3.3728007 ])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NullModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, Xtrain, Ytrain):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        length = X.shape[0]\n",
    "        return [0 for x in range(0, length)]\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {}\n",
    "    \n",
    "nulmod = null_model()\n",
    "\n",
    "cross_val_score(nulmod, X, Y, scoring=ll_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93352321,  0.93186228,  0.92629108])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mod, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09636832166591898"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_features(df, X):\n",
    "    mean_ll = []\n",
    "    null_ll = []\n",
    "    mean_acc = []\n",
    "    maj_class = []\n",
    "    for cls in classes:\n",
    "        Y = df[cls]\n",
    "        mod = MultinomialNB()\n",
    "        mean_ll.append(cross_val_score(mod, X, Y, scoring=ll_scorer, cv=5).mean())\n",
    "        nulmod = NullModel()\n",
    "        null_ll.append(cross_val_score(nulmod, X, Y, scoring=ll_scorer).mean())\n",
    "        mean_acc.append(cross_val_score(mod, X, Y, cv=5).mean())\n",
    "        maj_class.append(1-(Y.sum()/len(df)))\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "    result['class'] = classes\n",
    "    result['mean_log_loss'] = mean_ll\n",
    "    result['null_log_loss'] = null_ll\n",
    "    result['mean_accuracy'] = mean_acc\n",
    "    result['majority_class_prior'] = maj_class\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:  1\n",
      "vectorizing...\n",
      "testing...\n",
      "done!\n",
      "round:  2\n",
      "vectorizing...\n",
      "testing...\n",
      "done!\n",
      "round:  3\n",
      "vectorizing...\n",
      "testing...\n",
      "done!\n",
      "round:  4\n",
      "vectorizing...\n",
      "testing...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "result_ll = []\n",
    "ngram_max = []\n",
    "for x in range(1, 5):\n",
    "    gc.collect()\n",
    "    print(\"round: \", x)\n",
    "    mdf = x\n",
    "    ngram_max.append(mdf)\n",
    "    vctr = CountVectorizer(ngram_range=(1, mdf), stop_words = 'english', min_df=.0005, max_df=.5)\n",
    "    print(\"vectorizing...\")\n",
    "    X = vctr.fit_transform(data.clean_text)\n",
    "    print(\"testing...\")\n",
    "    result = test_features(df, X)\n",
    "    result_ll.append(result.mean_log_loss.mean())\n",
    "    print(\"done!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ll     1.016469\n",
       "mdf    1.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rezz = pd.DataFrame()\n",
    "rezz['ll'] = result_ll\n",
    "rezz['mdf'] = ngram_max\n",
    "rezz.loc[rezz.mdf.idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2684722758112856"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.mean_log_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2728948663912809"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.null_log_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>mean_log_loss</th>\n",
       "      <th>null_log_loss</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>majority_class_prior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>1.899363</td>\n",
       "      <td>3.328444</td>\n",
       "      <td>0.945008</td>\n",
       "      <td>0.903632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.644657</td>\n",
       "      <td>0.347727</td>\n",
       "      <td>0.981336</td>\n",
       "      <td>0.989932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>1.146612</td>\n",
       "      <td>1.840968</td>\n",
       "      <td>0.966803</td>\n",
       "      <td>0.946699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.339445</td>\n",
       "      <td>0.109903</td>\n",
       "      <td>0.990172</td>\n",
       "      <td>0.996818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>1.379034</td>\n",
       "      <td>1.717012</td>\n",
       "      <td>0.960073</td>\n",
       "      <td>0.950287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.689701</td>\n",
       "      <td>0.293315</td>\n",
       "      <td>0.980031</td>\n",
       "      <td>0.991508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class  mean_log_loss  null_log_loss  mean_accuracy  \\\n",
       "0          toxic       1.899363       3.328444       0.945008   \n",
       "1   severe_toxic       0.644657       0.347727       0.981336   \n",
       "2        obscene       1.146612       1.840968       0.966803   \n",
       "3         threat       0.339445       0.109903       0.990172   \n",
       "4         insult       1.379034       1.717012       0.960073   \n",
       "5  identity_hate       0.689701       0.293315       0.980031   \n",
       "\n",
       "   majority_class_prior  \n",
       "0              0.903632  \n",
       "1              0.989932  \n",
       "2              0.946699  \n",
       "3              0.996818  \n",
       "4              0.950287  \n",
       "5              0.991508  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "vctr = CountVectorizer(ngram_range=(1, 1), stop_words = 'english', min_df=.0005, max_df=.5)\n",
    "X = vctr.fit_transform(data.clean_text)\n",
    "test_features(df, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english', min_df=.0005, max_df=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing...\n",
      "testing features...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>mean_log_loss</th>\n",
       "      <th>null_log_loss</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>majority_class_prior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>1.783316</td>\n",
       "      <td>3.328444</td>\n",
       "      <td>0.948368</td>\n",
       "      <td>0.903632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.341601</td>\n",
       "      <td>0.347727</td>\n",
       "      <td>0.990110</td>\n",
       "      <td>0.989932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.963907</td>\n",
       "      <td>1.840968</td>\n",
       "      <td>0.972092</td>\n",
       "      <td>0.946699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.110263</td>\n",
       "      <td>0.109903</td>\n",
       "      <td>0.996808</td>\n",
       "      <td>0.996818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>1.154168</td>\n",
       "      <td>1.717012</td>\n",
       "      <td>0.966584</td>\n",
       "      <td>0.950287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.287910</td>\n",
       "      <td>0.293315</td>\n",
       "      <td>0.991664</td>\n",
       "      <td>0.991508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class  mean_log_loss  null_log_loss  mean_accuracy  \\\n",
       "0          toxic       1.783316       3.328444       0.948368   \n",
       "1   severe_toxic       0.341601       0.347727       0.990110   \n",
       "2        obscene       0.963907       1.840968       0.972092   \n",
       "3         threat       0.110263       0.109903       0.996808   \n",
       "4         insult       1.154168       1.717012       0.966584   \n",
       "5  identity_hate       0.287910       0.293315       0.991664   \n",
       "\n",
       "   majority_class_prior  \n",
       "0              0.903632  \n",
       "1              0.989932  \n",
       "2              0.946699  \n",
       "3              0.996818  \n",
       "4              0.950287  \n",
       "5              0.991508  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(\"vectorizing...\")\n",
    "X = tfidf.fit_transform(data.clean_text)\n",
    "print(\"testing features...\")\n",
    "test_features(df, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77352750511435786"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test_features(df, X)\n",
    "result.mean_log_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Out of Box\" Performance of CountVectorizer and Tfidf <br>\n",
    "Out of the box, the CountVectorizer improved very little on the performance of the \"null\" model (just predicting the majority class all the time.) After tinkering with hyperparameters, the model improved by quite a bit from a null log loss of 1.27 to 1.02 (not great, but a marked improvement).\n",
    "<br>\n",
    "Tf-idf \"out of the box\" gave a log loss score of .77, which is a huge improvement; still not awesome, but it's evidence of Tf-idf's general aplicability. The only class it wasn't able to predict very well was \"threat,\" which is an extremely rare class (99.7% of documents are not threats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Environment (conda_dspy3)",
   "language": "python",
   "name": "conda_dspy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
